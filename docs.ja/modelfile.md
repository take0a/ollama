# Ollama Model File

> [!NOTE]
> `Modelfile` 構文は現在開発中です。

モデルファイルは、Ollama でモデルを作成および共有するための設計図です。

## Table of Contents

- [Format](#format)
- [Examples](#examples)
- [Instructions](#instructions)
  - [FROM (Required)](#from-required)
    - [Build from existing model](#build-from-existing-model)
    - [Build from a Safetensors model](#build-from-a-safetensors-model)
    - [Build from a GGUF file](#build-from-a-gguf-file)
  - [PARAMETER](#parameter)
    - [Valid Parameters and Values](#valid-parameters-and-values)
  - [TEMPLATE](#template)
    - [Template Variables](#template-variables)
  - [SYSTEM](#system)
  - [ADAPTER](#adapter)
  - [LICENSE](#license)
  - [MESSAGE](#message)
- [Notes](#notes)

## Format

`Modelfile` の形式:

```
# comment
INSTRUCTION arguments
```

| Instruction                         | Description        |
| ----------------------------------- | ----------------- |
| [`FROM`](#from-required) (必須) | 使用するベースモデルを定義します。 |
| [`PARAMETER`](#parameter) | Ollama がモデルを実行する方法のパラメータを設定します。 |
| [`TEMPLATE`](#template) | モデルに送信される完全なプロンプトテンプレート。 |
| [`SYSTEM`](#system) | テンプレートに設定されるシステムメッセージを指定します。 |
| [`ADAPTER`](#adapter) | モデルに適用する (Q)LoRA アダプターを定義します。 |
| [`LICENSE`](#license) | 法的ライセンスを指定します。 |
| [`MESSAGE`](#message) | メッセージ履歴を指定します。 |

## Examples

### 基本的な `Modelfile`

mario の設計図を作成する `Modelfile` の例:

```
FROM llama3.2
# sets the temperature to 1 [higher is more creative, lower is more coherent]
PARAMETER temperature 1
# sets the context window size to 4096, this controls how many tokens the LLM can use as context to generate the next token
PARAMETER num_ctx 4096

# sets a custom system message to specify the behavior of the chat assistant
SYSTEM You are Mario from super mario bros, acting as an assistant.
```

使い方：

1. ファイルとして保存します（例：`Modelfile`）。
2. `ollama create choose-a-model-name -f <ファイルの場所（例：./Modelfile>）`
3. `ollama run choose-a-model-name`
4. モデルの使用を開始します！

特定のモデルのModelfileを表示するには、`ollama show --modelfile` コマンドを使用します。

```shell
ollama show --modelfile llama3.2
```

> **Output**:
>
> ```
> # Modelfile generated by "ollama show"
> # To build a new Modelfile based on this one, replace the FROM line with:
> # FROM llama3.2:latest
> FROM /Users/pdevine/.ollama/models/blobs/sha256-00e1317cbf74d901080d7100f57580ba8dd8de57203072dc6f668324ba545f29
> TEMPLATE """{{ if .System }}<|start_header_id|>system<|end_header_id|>
>
> {{ .System }}<|eot_id|>{{ end }}{{ if .Prompt }}<|start_header_id|>user<|end_header_id|>
>
> {{ .Prompt }}<|eot_id|>{{ end }}<|start_header_id|>assistant<|end_header_id|>
>
> {{ .Response }}<|eot_id|>"""
> PARAMETER stop "<|start_header_id|>"
> PARAMETER stop "<|end_header_id|>"
> PARAMETER stop "<|eot_id|>"
> PARAMETER stop "<|reserved_special_token"
> ```


## Instructions

### FROM (Required)

`FROM` 命令は、モデルを作成するときに使用する基本モデルを定義します。

```
FROM <model name>:<tag>
```

#### 既存のモデルから構築

```
FROM llama3.2
```

A list of available base models:
<https://github.com/ollama/ollama#model-library>
Additional models can be found at:
<https://ollama.com/library>

#### Safetensorsモデルから構築する

```
FROM <model directory>
```

モデルディレクトリには、サポートされているアーキテクチャの Safetensors の重みが含まれている必要があります。

現在サポートされているモデルアーキテクチャ:
  * Llama (including Llama 2, Llama 3, Llama 3.1, and Llama 3.2)
  * Mistral (including Mistral 1, Mistral 2, and Mixtral)
  * Gemma (including Gemma 1 and Gemma 2)
  * Phi3

#### GGUFファイルからビルドする

```
FROM ./ollama-model.gguf
```

GGUF ファイルの場所は、絶対パスまたは `Modelfile` の場所に対する相対パスとして指定する必要があります。


### PARAMETER

`PARAMETER` 命令は、モデルの実行時に設定できるパラメータを定義します。

```
PARAMETER <parameter> <parametervalue>
```

#### 有効なパラメータと値

| Parameter      | Description                                                                                                                                                                                                                                             | Value Type | Example Usage        |
| -------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------- | -------------------- |
| num_ctx | 次のトークンを生成するために使用されるコンテキスト ウィンドウのサイズを設定します。(デフォルト: 2048) | int | num_ctx 4096 |
| repeat_last_n | 繰り返しを防ぐためにモデルがどれだけ過去までさかのぼるかを設定します。(デフォルト: 64、0 = 無効、-1 = num_ctx) | int | repeat_last_n 64 |
| repeat_penalty | 繰り返しに対するペナルティの強さを設定します。値が高いほど (例: 1.5) 繰り返しに対するペナルティが強くなり、値が低いほど (例: 0.9) 繰り返しに対するペナルティが緩くなります。(デフォルト: 1.1) | float | repeat_penalty 1.1 |
| temperature | モデルの温度。温度を上げると、モデルはより創造的に回答するようになります。(デフォルト: 0.8) | float | temperature 0.7 |
| seed | 生成に使用する乱数シードを設定します。これを特定の数値に設定すると、モデルは同じプロンプトに対して同じテキストを生成します。(デフォルト: 0) | int | seed 42 |
| stop | 使用する停止シーケンスを設定します。このパターンに遭遇すると、LLM はテキストの生成を停止して戻ります。モデルファイルで複数の個別の `stop` パラメータを指定することで、複数の停止パターンを設定できます。 | string | stop "AI assistant:" |
| num_predict | テキスト生成時に予測するトークンの最大数。(デフォルト: -1、無限生成) | int | num_predict 42 |
| top_k | 意味不明な回答が生成される確率を減らします。値が大きいほど(例: 100)、回答の多様性が増し、値が低いほど(例: 10)、回答の安定性が増します。(デフォルト: 40) | int | top_k 40 |
| top_p | top-k と連動します。値が高いほど (例: 0.95) 多様なテキストが生成され、値が低いほど (例: 0.5) 焦点が絞られ保守的なテキストが生成されます。(既定値: 0.9) | float | top_p 0.9 |
| min_p | top_p の代替であり、品質と多様性のバランスを確保することを目的としています。パラメーター *p* は、最も可能性の高いトークンの確率に対する、トークンが考慮される最小確率を表します。たとえば、*p*=0.05 で、最も可能性の高いトークンの確率が 0.9 の場合、値が 0.045 未満のロジットは除外されます。(既定値: 0.0) | float | min_p 0.05 |

### TEMPLATE

モデルに渡される完全なプロンプトテンプレートの `TEMPLATE` です。システムメッセージ、ユーザーメッセージ、モデルからのレスポンス（オプション）を含めることができます。注: 構文はモデルによって異なる場合があります。テンプレートはGoの[テンプレート構文](https://pkg.go.dev/text/template)を使用します。

#### テンプレート変数

| Variable          | Description                                                                                   |
| ----------------- | --------------------------------------------------------------------------------------------- |
| `{{ .System }}` | カスタム動作を指定するために使用されるシステムメッセージ。|
| `{{ .Prompt }}` | ユーザープロンプトメッセージ。|
| `{{ .Response }}` | モデルからの応答。応答を生成する際、この変数の後のテキストは省略されます。|

```
TEMPLATE """{{ if .System }}<|im_start|>system
{{ .System }}<|im_end|>
{{ end }}{{ if .Prompt }}<|im_start|>user
{{ .Prompt }}<|im_end|>
{{ end }}<|im_start|>assistant
"""
```

### SYSTEM

`SYSTEM` 命令は、該当する場合にテンプレートで使用されるシステム メッセージを指定します。

```
SYSTEM """<system message>"""
```

### ADAPTER

`ADAPTER`命令は、ベースモデルに適用する微調整されたLoRAアダプタを指定します。アダプタの値は絶対パス、またはモデルファイルからの相対パスで指定します。ベースモデルは`FROM`命令で指定する必要があります。ベースモデルがアダプタの調整元となるベースモデルと異なる場合、動作が不安定になります。

#### Safetensor adapter

```
ADAPTER <path to safetensor adapter>
```

Currently supported Safetensor adapters:
  * Llama (including Llama 2, Llama 3, and Llama 3.1)
  * Mistral (including Mistral 1, Mistral 2, and Mixtral)
  * Gemma (including Gemma 1 and Gemma 2)

#### GGUF adapter

```
ADAPTER ./ollama-lora.gguf
```

### LICENSE

`LICENSE` 命令を使用すると、このモデルファイルで使用されるモデルを共有または配布する場合の法的ライセンスを指定できます。

```
LICENSE """
<license text>
"""
```

### MESSAGE

`MESSAGE`命令を使用すると、モデルが応答時に使用するメッセージ履歴を指定できます。MESSAGEコマンドを複数回繰り返すことで、モデルが同様の回答をするように誘導する会話を構築できます。

```
MESSAGE <role> <message>
```

#### 有効なロール

| ロール | 説明 |
| --------- | ------------------------------------------------------------ |
| system | モデルの SYSTEM メッセージを提供する別の方法。 |
| user | ユーザーが尋ねた可能性のあるメッセージの例。 |
| assistant | モデルが応答するべきメッセージの例。 |


#### 会話例

```
MESSAGE user Is Toronto in Canada?
MESSAGE assistant yes
MESSAGE user Is Sacramento in Canada?
MESSAGE assistant no
MESSAGE user Is Ontario in Canada?
MESSAGE assistant yes
```


## 注記

- **`Modelfile` は大文字と小文字を区別しません**。例では、引数と区別しやすくするために、大文字の命令を使用しています。
- 命令の順序は任意です。例では、読みやすさを考慮して `FROM` 命令を先頭にしています。

[1]: https://ollama.com/library
